{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.io as pio\n",
    "# setting Jedha color palette as default\n",
    "# pio.templates[\"jedha\"] = go.layout.Template(\n",
    "#     layout_colorway=[\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    "# )\n",
    "# pio.templates.default = \"jedha\"\n",
    "# pio.renderers.default = \"svg\" # to be replaced by \"iframe\" if working on JULIE\n",
    "# from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filename = \"v6\"\n",
    "enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        country  age  new_user source  total_pages_visited  converted\n",
      "11331        UK  111         0    Ads                   10          1\n",
      "233196  Germany  123         0    Seo                   15          1\n",
      "Empty DataFrame\n",
      "Columns: [country, age, new_user, source, total_pages_visited, converted]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('conversion_data_train.csv')\n",
    "\n",
    "# dropping outliers\n",
    "\n",
    "print(data[data.age > 100])\n",
    "data = data.drop(data[data.age > 100].index)\n",
    "print(data[data.age > 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['converted']\n",
    "X = data.drop('converted', axis=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=1337, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = ['age', 'total_pages_visited']\n",
    "cat_feat = ['country', 'new_user', 'source']\n",
    "\n",
    "# Create pipeline for numeric features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "#    ('imputer', KNNImputer()), \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "#    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n",
    "    ('encoder', OneHotEncoder(drop='first')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "# Use ColumnTransformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_feat),\n",
    "        ('cat', categorical_transformer, cat_feat)\n",
    "    ])\n",
    "\n",
    "# Preprocessings on train set\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Preprocessings on test set\n",
    "\n",
    "X_test = preprocessor.transform(X_test) # Don't fit again !! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search...\n",
      "{'estimator__C': [0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0], 'n_estimators': [5, 10, 20, 40, 60, 80, 100]}\n",
      "Fitting 3 folds for each of 56 candidates, totalling 168 fits\n",
      "...Done.\n",
      "Best hyperparameters :  {'estimator__C': 5.0, 'n_estimators': 60}\n",
      "Best validation accuracy :  0.9861588292513522\n",
      "\n",
      "Accuracy on training set :  0.9861471185381853\n",
      "Accuracy on test set :  0.986682128048352\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "# classifier = RandomForestClassifier()\n",
    "# classifier.fit(X_train, Y_train)\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Grid search...\")\n",
    "logistic_regression = LogisticRegression(max_iter = 1000)\n",
    "adaboost_logreg = AdaBoostClassifier(logistic_regression)\n",
    "\n",
    "# Grid of values to be tested\n",
    "params = {\n",
    "    'estimator__C': [0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0],\n",
    "    'n_estimators': [5, 10, 20, 40, 60, 80, 100]\n",
    "}\n",
    "print(params)\n",
    "gridsearch = GridSearchCV(adaboost_logreg, param_grid = params, cv = 3, verbose = 1) # cv : the number of folds to be used for CV\n",
    "gridsearch.fit(X_train, Y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", gridsearch.best_params_)\n",
    "print(\"Best validation accuracy : \", gridsearch.best_score_)\n",
    "print()\n",
    "print(\"Accuracy on training set : \", gridsearch.score(X_train, Y_train))\n",
    "print(\"Accuracy on test set : \", gridsearch.score(X_test, Y_test))\n",
    "\n",
    "classifier = gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "Y_train_pred = classifier.predict(X_train)\n",
    "Y_test_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score on train set :  0.7616552465403735\n",
      "f1-score on test set :  0.7720986169573061\n",
      "Confusion matrix on train set : \n",
      "[[246903    957]\n",
      " [  2591   5669]]\n",
      "\n",
      "Confusion matrix on test set : \n",
      "[[27437   103]\n",
      " [  276   642]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assessment\n",
    "\n",
    "print(\"f1-score on train set : \", f1_score(Y_train, Y_train_pred))\n",
    "print(\"f1-score on test set : \", f1_score(Y_test, Y_test_pred))\n",
    "\n",
    "# Confusion Matrices\n",
    "\n",
    "print(\"Confusion matrix on train set : \")\n",
    "print(confusion_matrix(Y_train, Y_train_pred))\n",
    "print()\n",
    "print(\"Confusion matrix on test set : \")\n",
    "print(confusion_matrix(Y_test, Y_test_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Reg de base\n",
    "\n",
    "f1-score on train set :  0.7626599147121536\n",
    "f1-score on test set :  0.7788944723618091\n",
    "Confusion matrix on train set : \n",
    "[[246837    985]\n",
    " [  2577   5723]]\n",
    "\n",
    "Confusion matrix on test set : \n",
    "[[27486    92]\n",
    " [  260   620]]\n",
    "\n",
    "## Log Reg stratifi√©\n",
    "\n",
    "f1-score on train set :  0.7645759421648034\n",
    "f1-score on test set :  0.7568223165554882\n",
    "Confusion matrix on train set : \n",
    "[[246894    966]\n",
    " [  2551   5711]]\n",
    "\n",
    "Confusion matrix on test set : \n",
    "[[27433   107]\n",
    " [  294   624]]\n",
    "\n",
    " ## Dec Tree de base\n",
    "\n",
    " f1-score on train set :  0.7949955803358943\n",
    "f1-score on test set :  0.7459066100667071\n",
    "Confusion matrix on train set : \n",
    "[[247259    601]\n",
    " [  2414   5846]]\n",
    "\n",
    "Confusion matrix on test set : \n",
    "[[27424   116]\n",
    " [  303   615]]\n",
    "\n",
    "## Random Forest de base\n",
    "\n",
    "f1-score on train set :  0.7993611499301257\n",
    "f1-score on test set :  0.759433962264151\n",
    "Confusion matrix on train set : \n",
    "[[247099    761]\n",
    " [  2254   6006]]\n",
    "\n",
    "Confusion matrix on test set : \n",
    "[[27406   134]\n",
    " [  274   644]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "-------\n",
    "\n",
    "## Model performance on official test data\n",
    "## Production of file to be scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model on whole data\n"
     ]
    }
   ],
   "source": [
    "# train model on whole data\n",
    "\n",
    "if enabled:\n",
    "    print(\"Fitting model on whole data\")\n",
    "    X_total = np.append(X_train,X_test,axis=0)\n",
    "    Y_total = np.append(Y_train,Y_test)\n",
    "\n",
    "    classifier.fit(X_total,Y_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test data for prediction\n"
     ]
    }
   ],
   "source": [
    "# Preparing test data\n",
    "\n",
    "if enabled:\n",
    "    print(\"Preparing test data for prediction\")\n",
    "    data_without_labels = pd.read_csv('conversion_data_test.csv')\n",
    "\n",
    "    # Warning : check consistency of features_list (must be the same than the features \n",
    "    # used by your best classifier)\n",
    "    features_list = num_feat + cat_feat\n",
    "    X_without_labels = data_without_labels[features_list]\n",
    "\n",
    "    X_without_labels = preprocessor.transform(X_without_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting test data and writing to file: conversion_data_test_predictions_guillaume-v6.csv\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and dump to file\n",
    "# WARNING : MAKE SURE THE FILE IS A CSV WITH ONE COLUMN NAMED 'converted' AND NO INDEX !\n",
    "# WARNING : FILE NAME MUST HAVE FORMAT 'conversion_data_test_predictions_[name].csv'\n",
    "# where [name] is the name of your team/model separated by a '-'\n",
    "# For example : [name] = AURELIE-model1\n",
    "\n",
    "if enabled:\n",
    "    data = {\n",
    "        'converted': classifier.predict(X_without_labels)\n",
    "    }\n",
    "\n",
    "    Y_predictions = pd.DataFrame(columns=['converted'],data=data)\n",
    "    filename = 'conversion_data_test_predictions_guillaume-' + my_filename + \".csv\"\n",
    "    print(\"Predicting test data and writing to file:\", filename)\n",
    "    Y_predictions.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
