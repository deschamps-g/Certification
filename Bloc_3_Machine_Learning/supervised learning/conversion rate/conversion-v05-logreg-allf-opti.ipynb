{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.io as pio\n",
    "# setting Jedha color palette as default\n",
    "# pio.templates[\"jedha\"] = go.layout.Template(\n",
    "#     layout_colorway=[\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    "# )\n",
    "# pio.templates.default = \"jedha\"\n",
    "# pio.renderers.default = \"svg\" # to be replaced by \"iframe\" if working on JULIE\n",
    "# from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filename = \"v5\"\n",
    "enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        country  age  new_user source  total_pages_visited  converted\n",
      "11331        UK  111         0    Ads                   10          1\n",
      "233196  Germany  123         0    Seo                   15          1\n",
      "Empty DataFrame\n",
      "Columns: [country, age, new_user, source, total_pages_visited, converted]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('conversion_data_train.csv')\n",
    "\n",
    "# dropping outliers\n",
    "\n",
    "print(data[data.age > 100])\n",
    "data = data.drop(data[data.age > 100].index)\n",
    "print(data[data.age > 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['converted']\n",
    "X = data.drop('converted', axis=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=1337, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = ['age', 'total_pages_visited']\n",
    "cat_feat = ['country', 'new_user', 'source']\n",
    "\n",
    "# Create pipeline for numeric features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "#    ('imputer', KNNImputer()), \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "#    ('imputer', SimpleImputer(strategy='most_frequent')), # missing values will be replaced by most frequent value\n",
    "#    ('encoder', OneHotEncoder(drop='first')) # first column will be dropped to avoid creating correlations between features\n",
    "    ('encoder', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "# Use ColumnTransformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_feat),\n",
    "        ('cat', categorical_transformer, cat_feat)\n",
    "    ])\n",
    "\n",
    "# Preprocessings on train set\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Preprocessings on test set\n",
    "\n",
    "X_test = preprocessor.transform(X_test) # Don't fit again !! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 88 candidates, totalling 264 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "99 fits failed out of a total of 264.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "33 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "                         ~~^~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "33 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "33 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Roumegaire\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.76245598        nan 0.76245598 0.76245598\n",
      " 0.76245598 0.76245598        nan        nan 0.76245598        nan\n",
      " 0.76245598 0.76245598 0.76245598 0.76245598        nan        nan\n",
      " 0.76245598        nan 0.76245598 0.76245598 0.76245598 0.76245598\n",
      "        nan        nan 0.76242614        nan 0.76245598 0.76245598\n",
      " 0.76245598 0.76245598        nan        nan 0.76245598        nan\n",
      " 0.76245598 0.76245598 0.76237322 0.76245598        nan        nan\n",
      " 0.76245598        nan 0.76245598 0.76245598 0.76245598 0.76245598\n",
      "        nan        nan 0.76245598        nan 0.76245598 0.76245598\n",
      " 0.76245598 0.76245598        nan        nan 0.76245598        nan\n",
      " 0.76245598 0.76245598 0.76245598 0.76245598        nan        nan\n",
      " 0.76245598        nan 0.76245598 0.76245598 0.76245598 0.76245598\n",
      "        nan        nan 0.76138081        nan 0.76061995 0.76061995\n",
      " 0.76245598 0.76245598        nan        nan 0.75429893        nan\n",
      " 0.75326961 0.75326961 0.76245598 0.76245598]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.762456 using {'C': 10000.0, 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Best: 0.762456 using {'C': 1000.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
    "# Best: 0.762456 using {'C': 1000.0, 'penalty': 'l1', 'solver': 'saga'}\n",
    "# Best: 0.762456 using {'C': 10000.0, 'penalty': 'l1', 'solver': 'saga'}\n",
    "\n",
    "# solvers = ['newton-cg', 'lbfgs', 'liblinear', 'saga']\n",
    "# penalty = ['l2', 'elasticnet', 'none']\n",
    "# c_val = [100., 10., 1., 0.1, 0.01]\n",
    "solvers = ['saga', 'newton-cg']\n",
    "penalty = ['elasticnet', 'l1', 'l2', None]\n",
    "c_val = [10_000., 5_000., 2_000., 1_000., 500., 200., 100., 50., 10., 0.1, 0.01]\n",
    "params = dict(solver=solvers, penalty=penalty, C=c_val)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=params, n_jobs=-1, cv=3, scoring='f1', verbose=2)\n",
    "grid_result = grid_search.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "classifier = grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "Y_train_pred = classifier.predict(X_train)\n",
    "Y_test_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score on train set :  0.7623842126459928\n",
      "f1-score on test set :  0.7743865948533812\n",
      "Confusion matrix on train set : \n",
      "[[246901    959]\n",
      " [  2581   5679]]\n",
      "\n",
      "Confusion matrix on test set : \n",
      "[[27434   106]\n",
      " [  271   647]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assessment\n",
    "\n",
    "print(\"f1-score on train set : \", f1_score(Y_train, Y_train_pred))\n",
    "print(\"f1-score on test set : \", f1_score(Y_test, Y_test_pred))\n",
    "\n",
    "# Confusion Matrices\n",
    "\n",
    "print(\"Confusion matrix on train set : \")\n",
    "print(confusion_matrix(Y_train, Y_train_pred))\n",
    "print()\n",
    "print(\"Confusion matrix on test set : \")\n",
    "print(confusion_matrix(Y_test, Y_test_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.61368843,  2.5252222 , -3.57328993,  0.26281682,  0.11336857,\n",
       "        -0.2541927 , -0.86540835, -2.58580426, -1.06709957, -1.27911221,\n",
       "        -1.10495807]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Reg de base\n",
    "\n",
    "f1-score on train set :  0.7626599147121536\n",
    "f1-score on test set :  0.7788944723618091\n",
    "Confusion matrix on train set : \n",
    "[[246837    985]\n",
    " [  2577   5723]]\n",
    "\n",
    "Confusion matrix on test set : \n",
    "[[27486    92]\n",
    " [  260   620]]\n",
    "\n",
    "## Log Reg stratifi√©\n",
    "\n",
    "f1-score on train set :  0.7645759421648034\n",
    "f1-score on test set :  0.7568223165554882\n",
    "Confusion matrix on train set : \n",
    "[[246894    966]\n",
    " [  2551   5711]]\n",
    "\n",
    "Confusion matrix on test set : \n",
    "[[27433   107]\n",
    " [  294   624]]\n",
    "\n",
    " ## Dec Tree de base\n",
    "\n",
    " f1-score on train set :  0.7949955803358943\n",
    "f1-score on test set :  0.7459066100667071\n",
    "Confusion matrix on train set : \n",
    "[[247259    601]\n",
    " [  2414   5846]]\n",
    "\n",
    "Confusion matrix on test set : \n",
    "[[27424   116]\n",
    " [  303   615]]\n",
    "\n",
    "## Random Forest de base\n",
    "\n",
    "f1-score on train set :  0.7993611499301257\n",
    "f1-score on test set :  0.759433962264151\n",
    "Confusion matrix on train set : \n",
    "[[247099    761]\n",
    " [  2254   6006]]\n",
    "\n",
    "Confusion matrix on test set : \n",
    "[[27406   134]\n",
    " [  274   644]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "-------\n",
    "\n",
    "## Model performance on official test data\n",
    "## Production of file to be scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on whole data\n",
    "\n",
    "if enabled:\n",
    "    print(\"Fitting model on whole data\")\n",
    "    X_total = np.append(X_train,X_test,axis=0)\n",
    "    Y_total = np.append(Y_train,Y_test)\n",
    "\n",
    "    classifier.fit(X_total,Y_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing test data\n",
    "\n",
    "if enabled:\n",
    "    print(\"Preparing test data for prediction\")\n",
    "    data_without_labels = pd.read_csv('conversion_data_test.csv')\n",
    "\n",
    "    # Warning : check consistency of features_list (must be the same than the features \n",
    "    # used by your best classifier)\n",
    "    features_list = num_feat + cat_feat\n",
    "    X_without_labels = data_without_labels[features_list]\n",
    "\n",
    "    X_without_labels = preprocessor.transform(X_without_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and dump to file\n",
    "# WARNING : MAKE SURE THE FILE IS A CSV WITH ONE COLUMN NAMED 'converted' AND NO INDEX !\n",
    "# WARNING : FILE NAME MUST HAVE FORMAT 'conversion_data_test_predictions_[name].csv'\n",
    "# where [name] is the name of your team/model separated by a '-'\n",
    "# For example : [name] = AURELIE-model1\n",
    "\n",
    "if enabled:\n",
    "    data = {\n",
    "        'converted': classifier.predict(X_without_labels)\n",
    "    }\n",
    "\n",
    "    Y_predictions = pd.DataFrame(columns=['converted'],data=data)\n",
    "    filename = 'conversion_data_test_predictions_guillaume-' + my_filename + \".csv\"\n",
    "    print(\"Predicting test data and writing to file:\", filename)\n",
    "    Y_predictions.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
